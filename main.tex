\documentclass{article} 

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[margin=1in]{geometry}
\usepackage[numbers]{natbib}

\newcommand{\R}{\mathbb R} 
\newcommand{\dv}[2]{\frac{d #1}{d #2}}
\newcommand{\pdv}[2]{\frac{\partial #1}{\partial #2}}

\usepackage{xcolor}

\title{Momentum Methods}
\author{Zachary Ross}

\begin{document}

\maketitle

In the following writeup, we explore accelerated gradient descent (GD) methods. These
are adaptations to the general method
\begin{equation}
    \label{eq:gd}
    x_{t + 1} = x_t - \eta \nabla f(x_t),
\end{equation} where $\eta$ is the learning rate and $f: \R^n \rightarrow \R$ is
a \emph{convex} objective function with a unique minimum, which use previous
descent directions as part of current iterates computation. The intuition behind
using accelerated methods is that it introduces a physics-inspired component to
GD which mimics a dampened oscillator ordinary differential equation (ODE). This
effect minimizes the oscillations that tend to occur in gradient descent by
using previous gradient computations to penalize rapid changes in direction and
reward movement towards an optimal point.

We reconstruct this via a method similar to as was done by
\citeauthor{10.1214/18-EJS1395}: the ODE limit for gradient descent is given by
the equation
\begin{equation}
    \dv{x}{t} = -\nabla f(x).
\end{equation}
This can be modified into a dampened oscillator ODE by including acceleration
and dampening term $\gamma \geq 0$
\begin{equation}
    \dv{^2 x}{t^2}  + \gamma \dv{x}{t} = -\nabla f(x)
\end{equation}
and then rearranged to interpret in terms of acceleration, along with a
simplification to velocity $v$ of $x$, i.e.  $v = \dv{x}{t}$,
\begin{equation}
    \dv{v}{t}  =  -\gamma v -\nabla f(x).
\end{equation}
Conversion to a discrete time step $\sqrt{\eta}$ with a right-derivative yields the equations
\begin{equation}
    \label{eq:der}
    \dv{v_t}{t} \approx \frac{v_{t + 1} - v_t}{\sqrt{\eta}}, \quad \text{and}
    \quad v_{t} = \dv{x_t}{t} \approx \frac{x_{t + 1} - x_t}{\sqrt{\eta}}
\end{equation}
which can be used to derive a formula for the dampened oscillator GD. Let $\beta
= 1 - \gamma \sqrt{\eta}$. Then 
\begin{equation}
    \label{eq:formula}
    \begin{aligned}
        \frac{v_{t} - v_{t - 1}}{\sqrt{\eta}} &= -\gamma v_{t - 1} -\nabla
        f(x_{t - 1}) \\
        v_{t} &= \beta v_{t - 1} - \sqrt{\eta} \nabla f(x_{t - 1})\\
        \frac{x_{t + 1} - x_t}{\sqrt{\eta}} &= \beta \frac{x_{t}
        - x_{t - 1}}{\sqrt{\eta}} - \sqrt{\eta} \nabla f(x_{t - 1}) \\
        x_{t + 1} &= x_t + \beta (x_{t} - x_{t - 1}) - \eta \nabla f(x_{t - 1})
    \end{aligned}
\end{equation}
By modifying this equation to evaluate the gradient at $x_t$ rather
than $x_{t - 1}$, this yields the \emph{heavyball} method as was discovered by
\citeauthor{heavyball}
\begin{equation}
    \label{eq:heavyball}
        x_{t + 1} = x_t + \beta (x_{t} - x_{t - 1}) - \eta \nabla f(x_t).
\end{equation}
The update to this method


Despite the effectiveness of this method, Polyak's still has the potential to
oscillate infinitely under specific conditions, as observed by
\citeauthor{lessard2016analysis}. This is due to the gradient being evaluated
\emph{before} momentum is applied and $x_t$ sometimes serving as a poor approximate
for $x_{t - 1}$. \citeauthor{nesterov1983method} altered this method slightly by
evaluating the gradient \emph{after} momentum is applied, i.e.
\begin{equation}
    x_{t + 1} = x_t + \beta (x_t - x_{t - 1}) - \eta \nabla f(x_t + \beta (x_t - x_{t -
    1})) .
\end{equation}
This derivation can be made by reinterpreting Equation~\ref{eq:der} as a left
derivative with timestep $\sqrt\delta$
\begin{equation}
    \label{eq:der}
    \dv{v_t}{t} \rightarrow \frac{v_{t} - v_{t - 1}}{\sqrt{\delta}}, \quad \text{and}
    \quad v_{t} = \dv{x_t}{t} \rightarrow \frac{x_{t} - x_{t - 1}}{\sqrt{\delta}}
\end{equation}
and re-deriving Equation~\ref{eq:formula} under this condition for $v_{t + 1}$
\begin{equation}
    \label{eq:formula}
    \begin{aligned}
        \frac{v_{t + 1} - v_{t}}{\sqrt{\delta}} &= -\gamma v_{t + 1} -\nabla
        f(x_{t + 1}) \\
        v_{t + 1} - v_{t} &= -\gamma \sqrt{\delta}v_{t + 1} -\sqrt{\delta}\nabla
        f(x_{t + 1}) \\
        \frac{x_{t + 1} - x_{t} - x_{t} + x_{t - 1}}{\sqrt{\delta}} &= -\gamma
        \sqrt{\delta}\frac{x_{t + 1} - x_t}{\sqrt{\delta}} -\sqrt{\delta}\nabla
        f(x_{t + 1}) \\
        x_{t + 1} - x_{t} - x_{t} + x_{t - 1} &= -\gamma
        \sqrt{\delta}(x_{t + 1} - x_t) - \delta\nabla
        f(x_{t + 1}) \\
         x_{t + 1}  &= x_t + \beta(x_{t} - x_{t - 1}) - \beta\delta\nabla
        f(x_{t + 1}) \\
    \end{aligned}
\end{equation}

\begin{equation}
    \beta = \frac{1}{1 + \gamma \sqrt{\delta}}
\end{equation}


\section{Questions}%
\label{sec:questions}

\begin{enumerate}
    \item The derivation in Equation~\ref{eq:der} defines a right-derivative to
        be used in the computation, which 
\end{enumerate}



\bibliography{bib}
\bibliographystyle{plainnat}


\section{Closed Form}%
\label{sec:closed_form}

Conversion to a discrete time step $\sqrt{\eta}$ yields the equations
\begin{equation}
    \dv{v_t}{t} \rightarrow \frac{v_{t} - v_{t - 1}}{\sqrt{\eta}}, \quad \text{and}
    \quad v_{t} = \dv{x_t}{t} \rightarrow \frac{x_{t} - x_{t - 1}}{\sqrt{\eta}}
\end{equation}
which can be used to derive a formula for a dampened oscillator for GD. Let $\beta = 1 - \gamma \sqrt{\eta}$. Then 
\begin{equation}
    \begin{aligned}
        \frac{v_{t} - v_{t - 1}}{\sqrt{\eta}} &= -\gamma v_{t - 1} -\nabla
        f(x_t) \\
        v_{t} &= \beta v_{t - 1} - \sqrt{\eta} \nabla f(x_t)\\
        \frac{x_{t + 1} - x_t}{\sqrt{\eta}} &= \beta \frac{x_{t}
        - x_{t - 1}}{\sqrt{\eta}} - \sqrt{\eta} \nabla f(x_t).
    \end{aligned}
\end{equation}
This yields the \emph{heavyball} method as was discovered by
\citeauthor{heavyball}.
\begin{equation}
        x_{t + 1} = x_t + \beta (x_{t} - x_{t - 1}) - \eta \nabla f(x_t)
\end{equation}

\end{document}


