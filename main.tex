\documentclass{article} 

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[margin=1in]{geometry}
\usepackage[numbers]{natbib}
\usepackage{graphicx}

\newcommand{\R}{\mathbb R} 
\newcommand{\E}{\mathbb E} 
\newcommand{\dv}[2]{\frac{d #1}{d #2}}
\newcommand{\pdv}[2]{\frac{\partial #1}{\partial #2}}

\usepackage{xcolor}

\title{Accelerating Gradient Descent}
\author{Zachary Ross}

\begin{document}

\maketitle

In the following writeup, we explore accelerated gradient descent (GD) methods. These
are adaptations to the general method
\begin{equation}
    \label{eq:gd}
    x_{t + 1} = x_t - \eta \nabla f(x_t),
\end{equation} where $\eta$ is the learning rate and $f: \R^n \rightarrow \R$ is
a \emph{convex} objective function with a unique minimum, which use previous
descent directions as part of current iterates computation. 

\section{Accelerated Methods}

The intuition behind using accelerated methods is that it introduces a
physics-inspired component to GD which mimics a dampened oscillator ordinary
differential equation (ODE). This effect minimizes the oscillations that tend to
occur in gradient descent by using previous gradient computations to penalize
rapid changes in direction and reward movement towards an optimal point.

We reconstruct this via a method similar to as was done by
\citeauthor{10.1214/18-EJS1395}: the ODE limit for gradient descent is given by
the equation
\begin{equation}
    \dv{x}{t} = -\nabla f(x).
\end{equation}
This can be modified to mimic a dampened oscillator ODE by including acceleration
and dampening term $\gamma \geq 0$
\begin{equation}
    \dv{^2 x}{t^2}  + \gamma \dv{x}{t} = -\nabla f(x)
\end{equation}
and then rearranged to interpret in terms of acceleration, along with a
simplification to velocity $v$ of $x$, i.e.  $v = \dv{x}{t}$,
\begin{equation}
    \dv{v}{t}  =  -\gamma v -\nabla f(x).
\end{equation}
Conversion to a discrete time step $\sqrt{\eta}$ with a right-derivative yields the equations
\begin{equation}
    \label{eq:der}
    \dv{v_t}{t} \approx \frac{v_{t + 1} - v_t}{\sqrt{\eta}}, \quad \text{and}
    \quad v_{t} = \dv{x_t}{t} \approx \frac{x_{t + 1} - x_t}{\sqrt{\eta}}
\end{equation}
which can be used to derive a formula for the dampened oscillator GD. Let $\beta
= 1 - \gamma \sqrt{\eta}$. Then at iteration $t - 1$
\begin{equation}
    \label{eq:formula}
    \begin{aligned}
        \frac{v_{t} - v_{t - 1}}{\sqrt{\eta}} &= -\gamma v_{t - 1} -\nabla
        f(x_{t - 1}) \\
        v_{t} - v_{t - 1} &= -\gamma\sqrt\eta v_{t - 1} -\sqrt \eta \nabla
        f(x_{t - 1}) \\
        v_{t} &= \beta v_{t - 1} - \sqrt{\eta} \nabla f(x_{t - 1})\\
        \frac{x_{t + 1} - x_t}{\sqrt{\eta}} &= \beta \frac{x_{t}
        - x_{t - 1}}{\sqrt{\eta}} - \sqrt{\eta} \nabla f(x_{t - 1}) \\
        x_{t + 1} &= x_t + \beta (x_{t} - x_{t - 1}) - \eta \nabla f(x_{t - 1})
    \end{aligned}
\end{equation}
By modifying this equation to evaluate the gradient at $x_t$ rather
than $x_{t - 1}$, this yields the \emph{heavyball} method as was discovered by
\citeauthor{heavyball}
\begin{equation}
    \label{eq:heavyball}
        x_{t + 1} = x_t + \beta (x_{t} - x_{t - 1}) - \eta \nabla f(x_t).
\end{equation}


Despite the effectiveness of this method, Polyak's still has the potential to
oscillate infinitely under specific conditions, as observed by
\citeauthor{lessard2016analysis}. This is due to the gradient being evaluated
\emph{before} momentum is applied and $x_t$ sometimes serving as a poor approximate
for $x_{t - 1}$. \citeauthor{nesterov1983method} altered this method slightly by
evaluating the gradient \emph{after} momentum is applied.  This derivation can
be made by reinterpreting Equation~\ref{eq:der} as a left derivative with time
step $\sqrt\delta$ for some $\delta \geq 0$
\begin{equation}
    \label{eq:der}
    \dv{v_t}{t} \rightarrow \frac{v_{t} - v_{t - 1}}{\sqrt{\delta}}, \quad \text{and}
    \quad v_{t} = \dv{x_t}{t} \rightarrow \frac{x_{t} - x_{t - 1}}{\sqrt{\delta}}
\end{equation}
and re-deriving Equation~\ref{eq:formula} under this condition. Let $\beta =
{\left( 1 + \gamma \sqrt{\delta} \right) }^{-1}$ and $\eta = \beta \delta$. Then
at iteration $t + 1$
\begin{equation}
    \label{eq:formula}
    \begin{aligned}
        \frac{v_{t + 1} - v_{t}}{\sqrt{\delta}} &= -\gamma v_{t + 1} -\nabla
        f(x_{t + 1}) \\
        v_{t + 1} - v_{t} &= -\gamma \sqrt{\delta}v_{t + 1} -\sqrt{\delta}\nabla
        f(x_{t + 1}) \\
        \beta^{-1}v_{t + 1} - v_{t} &= -\sqrt{\delta}\nabla
        f(x_{t + 1}) \\
        \frac{x_{t + 1} - x_{t}}{\beta\sqrt{\delta}} - \frac{x_{t} - x_{t - 1}}{\sqrt{\delta}} &=  -\sqrt{\delta}\nabla
        f(x_{t + 1}) \\
        \frac{x_{t + 1} - x_{t}}{\beta\sqrt{\delta}} &= \frac{x_{t} - x_{t -
        1}}{\sqrt{\delta}} - \sqrt{\delta}\nabla f(x_{t + 1}) \\
        x_{t + 1} - x_{t} &= \beta (x_{t} - x_{t -
        1}) - \eta \nabla f(x_{t + 1}) \\
        x_{t + 1} &= x_{t} + \beta (x_{t} - x_{t -
        1}) - \eta \nabla f(x_{t + 1}).
    \end{aligned}
\end{equation}

By evaluating the gradient at $x_t + \beta(x_t - x_{t - 1})$ rather than $x_{t +
1}$, this then yields Nesterov's accelerated method.
\begin{equation}
    x_{t + 1} = x_t + \beta (x_t - x_{t - 1}) - \eta \nabla f(x_t + \beta (x_t - x_{t -
    1})).
\end{equation}
The difference between these methods is shown in
Figure~\ref{fig:sc1-png}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{sc1.png}
    \caption{Comparison between
    Polyak’s and Nesterov’s momentum. The gradient descent step (orange arrow)
    is perpendicular to the level set before applying momentum to $x_1$ (red
    arrow) in Polyak’s algorithm, whereas it is perpendicular to the level set
    after applying momentum to $x_1$ in Nesterov’s algorithm. Graphic provided by \citeauthor{nesterovnotes}.}
    \label{fig:sc1-png}
\end{figure}

\section{First Order Methods}

This section discusses methods of the form 
\begin{equation}
    x_{t + 1} = x_t - \eta H_t^{-1} \nabla f(x_t)
\end{equation}
where $H_t$ is a first-order hessian approximation.  Hessian approximation
methods make use of the geometry of the data to approximate the Fisher
information matrix, which is itself a hessian approximation, given by 
\begin{equation}
    \label{eq:fim}
    I(x) = \mathbb E_x \left[ \nabla f(x) \nabla {f(x)}^\intercal \right]
\end{equation}
to find a solution to the root-finding problem
\begin{equation}
    f(x^*) \approx f(x_t) + \nabla {f(x_t)}^\intercal (x^* - x_t) + \frac 1 2 {(x^* -
    x_t)}^\intercal H_t^{-1} (x^* - x_t).
\end{equation}
First order methods utilize this by approximating the diagonal of this matrix
via
\begin{equation}
    I(x) \approx \text{diag} \left\{ \mathbb E_x \left[ \nabla f(x) \odot \nabla
    {f(x)} \right] \right\}.
\end{equation}
rather than using expensive computations in Equation~\ref{eq:fim}.

\citeauthor{DBLP:journals/corr/KingmaB14} provide a method which combines
earlier methods AdaGrad and RMSProp to build \emph{Adam}, which is theoretically
defined for some approximate $g_t$ s.t. $\E [g_t] = \nabla f(x_t)$ using
\begin{equation}
    x_{t + 1} = x_t - \eta \frac{\E \left[ g_t \right]}{\sqrt{\E \left[
        {g_t}^2  \right] }}.
\end{equation}
This method can be seen as minimizing a \emph{Signal-to-Noise ratio}, as it
uses the gradient's first moment (mean) divided by the gradient's square root of
the second moment (un-adjusted standard deviation) as a descent direction. As
this value reaches an optima, the mean will tend to decline and become overcome
by the noise.

Both $\E \left[ g_t \right]$ and $\E \left[ {g_t}^2
\right]$ are approximated using exponential moving averages, where if $g_t \sim
\rho(g_t)$ is the gradient distribution and $g_t$ is selected s.t. $\E[g_t] =
\nabla f(x_t)$ then
\begin{equation}
    \begin{aligned}
        m_t &= \beta_1 m_{t-1} + (1 - \beta_1)g_{t}, \text{ and} \\
        v_t &= \beta_2 v_{t-1} + (1 - \beta_2)g_{t}.
    \end{aligned}
\end{equation}
where $m_0$ and $v_0$ are zero-value initialized. Due to this initialization,
both terms require bias correction, as we can see by taking the expectation of
the closed form solution to $v_t$ (the same applies to $m_t$)
\begin{equation}
    \label{eq:bias_correction}
    \begin{aligned}
        \E \left[ v_t \right] &= \E \left[ (1 - \beta_2) \sum_{i = 1}^t
        \beta_2^{t - i} g_i^2 \right] \\
         &= \E \left[g_t^2 \right](1 - \beta_2) \sum_{i = 1}^t
        \beta_2^{t - i} + \xi \\
         &= \E \left[g_t^2 \right](1 - \beta_2^t) + \xi.
    \end{aligned}
\end{equation}
The bias correction is then employed by including the bias adjustment term to
each equation
\begin{equation}
    \hat m_t = \frac{m_t}{1 - \beta_1^t}, \quad \text{and} \quad
    \hat v_t = \frac{v_t}{1 - \beta_2^t}
\end{equation}
which is substituted in the gradient descent equation to get 
\begin{equation}
    x_{t + 1} = x_t - \eta \frac{\hat m_t}{\sqrt{\hat v_t} + \epsilon}
\end{equation}
where $\epsilon \ll 1$ is used to ensure the denominator never reaches 0. Note
the use of $\xi$ in Equation~\ref{eq:bias_correction}. Although this means there
will never be 0 bias, the used of a $\beta$ large enough will ensure that this
value is sufficiently small in most cases.

\bibliography{bib}
\bibliographystyle{plainnat}



\end{document}


